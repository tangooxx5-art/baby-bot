import os
import json
import tempfile
import logging
import threading
import time
import base64
import requests

from flask import Flask, request, abort
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__)

# è¨­å®š logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get('LINE_CHANNEL_ACCESS_TOKEN', '')
LINE_CHANNEL_SECRET = os.environ.get('LINE_CHANNEL_SECRET', '')

# æ”¯æ´å¤šæŠŠ Gemini API Key è¼ªæ›¿ä½¿ç”¨ï¼ˆå‹•æ…‹æƒææ‰€æœ‰ GEMINI_API_KEY* ç’°å¢ƒè®Šæ•¸ï¼‰
GEMINI_API_KEYS = []
_key_names = ['GEMINI_API_KEY'] + [f'GEMINI_API_KEY_{i}' for i in range(2, 21)]
for key_name in _key_names:
    key = os.environ.get(key_name, '')
    if key:
        GEMINI_API_KEYS.append(key)
        logger.info(f"Loaded key from {key_name}")
logger.info(f"Total Gemini API keys loaded: {len(GEMINI_API_KEYS)}")


class QuotaExhaustedError(Exception):
    """æ‰€æœ‰ API Key é…é¡éƒ½å·²è€—ç›¡"""
    pass


_current_key_index = 0  # ç›®å‰ä½¿ç”¨çš„ Key ç´¢å¼•

# --- é€Ÿç‡é™åˆ¶ & å†·å»æ©Ÿåˆ¶ ---
_key_cooldown = {}          # {key_index: cooldown_until_timestamp}
_global_cooldown_until = 0  # æ‰€æœ‰ key éƒ½è€—ç›¡æ™‚çš„å…¨åŸŸå†·å»æˆªæ­¢æ™‚é–“
_last_request_time = 0      # ä¸Šæ¬¡ API è«‹æ±‚çš„æ™‚é–“æˆ³
_rate_lock = threading.Lock()  # ä¿è­·å…±äº«ç‹€æ…‹çš„é–

# å†·å»æ™‚é–“è¨­å®šï¼ˆç§’ï¼‰
PER_KEY_COOLDOWN = 60       # å–®æŠŠ key è¢« 429 å¾Œæš«åœ 60 ç§’
GLOBAL_COOLDOWN = 120       # æ‰€æœ‰ key éƒ½è€—ç›¡å¾Œæš«åœ 120 ç§’
MIN_REQUEST_INTERVAL = 2    # é€£çºŒ API è«‹æ±‚é–“æœ€å°‘é–“éš” 2 ç§’

# å»¶é²åˆå§‹åŒ–
line_configuration = None
line_handler = None

# å›ºå®šä½¿ç”¨çš„ Gemini æ¨¡å‹ï¼ˆä¸å†å‹•æ…‹åµæ¸¬ï¼Œç¯€çœ API é…é¡ï¼‰
GEMINI_MODEL = 'gemini-2.5-flash'

# --- OpenRouter å‚™æ´è¨­å®š ---
OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY', '')
OPENROUTER_BASE_URL = 'https://openrouter.ai/api/v1/chat/completions'
# å…è²» vision æ¨¡å‹ï¼ˆæŒ‰å„ªå…ˆé †åºå˜—è©¦ï¼‰
OPENROUTER_FREE_MODELS = [
    'qwen/qwen2.5-vl-32b-instruct:free',
    'meta-llama/llama-3.2-11b-vision-instruct:free',
    'google/gemma-3-4b-it:free',
]
if OPENROUTER_API_KEY:
    logger.info(f"OpenRouter fallback enabled with {len(OPENROUTER_FREE_MODELS)} free models")
else:
    logger.warning("OPENROUTER_API_KEY not set â€” fallback disabled")


def get_line_config():
    global line_configuration, line_handler
    if line_configuration is None:
        from linebot.v3.messaging import Configuration
        from linebot.v3 import WebhookHandler
        line_configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
        line_handler = WebhookHandler(LINE_CHANNEL_SECRET)
        _register_handlers()
    return line_configuration, line_handler


def _register_handlers():
    """è¨»å†Š LINE webhook äº‹ä»¶è™•ç†å™¨"""
    from linebot.v3.webhooks import MessageEvent, ImageMessageContent

    @line_handler.add(MessageEvent, message=ImageMessageContent)
    def handle_image_message(event):
        user_id = event.source.user_id
        message_id = event.message.id
        reply_token = event.reply_token
        thread = threading.Thread(
            target=_process_image_async,
            args=(user_id, message_id, reply_token)
        )
        thread.start()



@app.route("/", methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥è·¯ç”±"""
    return "Baby Bot is running! ğŸ¼"


@app.route("/callback", methods=['POST'])
def callback():
    from linebot.v3.exceptions import InvalidSignatureError

    signature = request.headers.get('X-Line-Signature', '')
    body = request.get_data(as_text=True)
    logger.info("Request body: " + body)

    _, handler = get_line_config()

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        logger.error("Invalid signature.")
        abort(400)
    except Exception as e:
        logger.error(f"Error in callback handler: {e}", exc_info=True)

    return 'OK'


def _is_in_global_cooldown():
    """æª¢æŸ¥æ˜¯å¦åœ¨å…¨åŸŸå†·å»æœŸå…§"""
    now = time.time()
    if now < _global_cooldown_until:
        remaining = int(_global_cooldown_until - now)
        logger.info(f"Global cooldown active, {remaining}s remaining")
        return True, remaining
    return False, 0


def _throttle_request():
    """ç¢ºä¿é€£çºŒè«‹æ±‚ä¹‹é–“æœ‰æœ€å°é–“éš”ï¼Œé¿å…ç¬é–“å¤§é‡å‘¼å«"""
    global _last_request_time
    with _rate_lock:
        now = time.time()
        elapsed = now - _last_request_time
        if elapsed < MIN_REQUEST_INTERVAL:
            wait = MIN_REQUEST_INTERVAL - elapsed
            logger.info(f"Throttling: waiting {wait:.1f}s before next API call")
            time.sleep(wait)
        _last_request_time = time.time()


def _call_gemini_with_rotation(genai, image_path, prompt, max_rounds=3):
    """ä½¿ç”¨å¤šæŠŠ API Key è¼ªæ›¿å‘¼å« Geminiï¼Œå«é€Ÿç‡é™åˆ¶ã€per-key å†·å»ã€æŒ‡æ•¸é€€é¿é‡è©¦"""
    global _current_key_index, _global_cooldown_until

    if not GEMINI_API_KEYS:
        raise ValueError("No Gemini API keys configured!")

    # 1. æª¢æŸ¥å…¨åŸŸå†·å»
    in_cooldown, remaining = _is_in_global_cooldown()
    if in_cooldown:
        raise QuotaExhaustedError(
            f"æ‰€æœ‰ API Key é…é¡è€—ç›¡ï¼Œå…¨åŸŸå†·å»ä¸­ï¼ˆå‰©é¤˜ {remaining} ç§’ï¼‰"
        )

    last_error = None

    for round_num in range(max_rounds):
        if round_num > 0:
            wait_seconds = min(15 * (2 ** (round_num - 1)), 60)  # 15s, 30s, 60s
            logger.info(f"All keys exhausted in round {round_num}, waiting {wait_seconds}s before retry...")
            time.sleep(wait_seconds)

        keys_tried = 0
        keys_in_cooldown = 0

        for attempt in range(len(GEMINI_API_KEYS)):
            key_index = (_current_key_index + attempt) % len(GEMINI_API_KEYS)
            now = time.time()

            # 2. æª¢æŸ¥æ­¤ key æ˜¯å¦åœ¨å€‹åˆ¥å†·å»æœŸ
            cooldown_until = _key_cooldown.get(key_index, 0)
            if now < cooldown_until:
                remaining_cd = int(cooldown_until - now)
                logger.info(f"Key #{key_index + 1} in cooldown ({remaining_cd}s left), skipping")
                keys_in_cooldown += 1
                continue

            keys_tried += 1
            api_key = GEMINI_API_KEYS[key_index]
            logger.info(f"[Round {round_num + 1}/{max_rounds}] Trying Key #{key_index + 1}/{len(GEMINI_API_KEYS)}")

            # 3. é™æµï¼šç¢ºä¿è«‹æ±‚é–“éš”
            _throttle_request()

            try:
                genai.configure(api_key=api_key)
                sample_file = genai.upload_file(path=image_path, display_name="Ultrasound")
                logger.info(f"Using model: {GEMINI_MODEL}")
                model = genai.GenerativeModel(GEMINI_MODEL)
                response = model.generate_content([sample_file, prompt])

                # æ¸…ç† Gemini æš«å­˜
                try:
                    genai.delete_file(sample_file.name)
                except Exception:
                    pass

                # æˆåŠŸï¼æ›´æ–°ç´¢å¼•åˆ°ä¸‹ä¸€æŠŠï¼Œæ¸…é™¤æ­¤ key çš„å†·å»
                _current_key_index = (key_index + 1) % len(GEMINI_API_KEYS)
                _key_cooldown.pop(key_index, None)
                return response

            except Exception as e:
                last_error = e
                error_str = str(e)
                if '429' in error_str or 'ResourceExhausted' in error_str or 'quota' in error_str.lower():
                    # 4. è¨˜éŒ„æ­¤ key çš„å†·å»æˆªæ­¢æ™‚é–“
                    _key_cooldown[key_index] = time.time() + PER_KEY_COOLDOWN
                    logger.warning(
                        f"Key #{key_index + 1} hit 429, cooldown {PER_KEY_COOLDOWN}s until "
                        f"{time.strftime('%H:%M:%S', time.localtime(_key_cooldown[key_index]))}"
                    )
                    continue
                else:
                    raise

        # å¦‚æœé€™ä¸€è¼ªæ‰€æœ‰ key éƒ½åœ¨å†·å»ä¸­ï¼ˆæ²’æœ‰å¯¦éš›å˜—è©¦ï¼‰ï¼Œç›´æ¥è·³å‡º
        if keys_tried == 0:
            logger.warning("All keys are in per-key cooldown, no keys available to try")
            break

    # 5. æ‰€æœ‰å˜—è©¦å¤±æ•— âœ å•Ÿå‹•å…¨åŸŸå†·å»ï¼Œé˜²æ­¢å¾ŒçºŒè«‹æ±‚ç¹¼çºŒé€£æ‰“
    _global_cooldown_until = time.time() + GLOBAL_COOLDOWN
    logger.error(
        f"All {len(GEMINI_API_KEYS)} keys exhausted after {max_rounds} rounds. "
        f"Global cooldown activated until {time.strftime('%H:%M:%S', time.localtime(_global_cooldown_until))}"
    )
    raise QuotaExhaustedError(
        f"æ‰€æœ‰ {len(GEMINI_API_KEYS)} æŠŠ API Key é…é¡è€—ç›¡ï¼Œå·²å•Ÿå‹• {GLOBAL_COOLDOWN} ç§’å…¨åŸŸå†·å»"
    )


def _call_openrouter_fallback(image_path, prompt):
    """ä½¿ç”¨ OpenRouter å…è²» vision æ¨¡å‹ä½œç‚ºå‚™æ´"""
    if not OPENROUTER_API_KEY:
        raise ValueError("OPENROUTER_API_KEY not configured")

    # å°‡åœ–ç‰‡è½‰ç‚º base64
    with open(image_path, 'rb') as f:
        image_b64 = base64.b64encode(f.read()).decode('utf-8')

    headers = {
        'Authorization': f'Bearer {OPENROUTER_API_KEY}',
        'Content-Type': 'application/json',
        'HTTP-Referer': 'https://baby-bot.onrender.com',
        'X-Title': 'Baby Bot',
    }

    messages = [
        {
            'role': 'user',
            'content': [
                {'type': 'text', 'text': prompt},
                {
                    'type': 'image_url',
                    'image_url': {
                        'url': f'data:image/jpeg;base64,{image_b64}'
                    }
                }
            ]
        }
    ]

    last_error = None
    for model in OPENROUTER_FREE_MODELS:
        logger.info(f"[OpenRouter] Trying model: {model}")
        try:
            resp = requests.post(
                OPENROUTER_BASE_URL,
                headers=headers,
                json={'model': model, 'messages': messages, 'max_tokens': 1024},
                timeout=60
            )

            if resp.status_code == 200:
                data = resp.json()
                text = data['choices'][0]['message']['content']
                logger.info(f"[OpenRouter] Success with {model}")
                return text
            else:
                logger.warning(f"[OpenRouter] {model} returned {resp.status_code}: {resp.text[:200]}")
                last_error = Exception(f"OpenRouter {resp.status_code}: {resp.text[:200]}")
                continue

        except Exception as e:
            logger.warning(f"[OpenRouter] {model} failed: {e}")
            last_error = e
            continue

    if last_error is not None:
        raise last_error
    raise Exception("All OpenRouter models failed")


# --- å…±ç”¨çš„ prompt ---
ANALYSIS_PROMPT = """
è«‹ä½œç‚ºä¸€åã€Œæš–å¿ƒå­•æœŸåŠ©ç†ã€ï¼Œè™•ç†å‚³å…¥çš„å½±åƒï¼š
- OCR æå–ï¼šè¾¨è­˜ GA (é€±æ•¸)ã€EFW (é«”é‡)ã€EDD (é ç”¢æœŸ)ã€‚
- èªå¢ƒç”Ÿæˆï¼š
  1. ä½¿ç”¨ã€Œç¬¬ä¸€äººç¨±å¯¶å¯¶èªæ°£ã€ï¼ˆä¾‹å¦‚ï¼šåª½å’ªï¼Œæˆ‘ä»Šå¤©...ï¼‰ã€‚
  2. å°‡é‡é‡èˆ‡æ°´æœ/é£Ÿç‰©å°æ¯”ï¼ˆå¦‚ï¼š200g = ä¸€é¡†å¤§è˜‹æœï¼‰ã€‚
  3. åµæ¸¬ç…§ç‰‡å…§å®¹ï¼ˆè‹¥æ˜¯ 3D è‡‰éƒ¨ï¼Œç¨±è®šé¼»å­æˆ–å˜´å·´ï¼›è‹¥æ˜¯é»‘ç™½ 2Dï¼Œå¼·èª¿å¿ƒè·³èˆ‡æˆé•·ï¼‰ã€‚
- è¼¸å‡ºé™åˆ¶ï¼šåƒ…è¼¸å‡º JSON æ ¼å¼ï¼ŒåŒ…å« `weeks`, `weight_status`, `message`, `suggested_color`ã€‚
è«‹å‹¿è¼¸å‡ºä»»ä½• markdown æ¨™è¨˜ï¼Œç›´æ¥è¼¸å‡ºä¹¾æ·¨çš„ JSON å­—ä¸²ã€‚
""".strip()


def _parse_ai_response(response_text):
    """è§£æ AI å›å‚³çš„ JSON æ–‡å­—"""
    text = response_text.strip()
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]

    try:
        return json.loads(text.strip())
    except json.JSONDecodeError as e:
        logger.error(f"JSON parse failed: {e}, raw: {text[:300]}")
        return {
            "weeks": "?",
            "message": text[:300] if text else "åª½å’ªå¥½ï¼æˆ‘çœ‹ä¸å¤ªæ¸…æ¥šï¼Œå¯ä»¥å†å‚³ä¸€æ¬¡æ¸…æ™°çš„ç…§ç‰‡å—ï¼Ÿ",
            "weight_status": "æœªçŸ¥",
            "suggested_color": "#ffcccc"
        }


def _process_image_async(user_id, message_id, reply_token):
    """åœ¨èƒŒæ™¯è™•ç†åœ–ç‰‡ â€” Gemini å„ªå…ˆï¼ŒOpenRouter å‚™æ´"""
    import google.generativeai as genai
    from linebot.v3.messaging import (
        ApiClient,
        MessagingApi,
        MessagingApiBlob,
        ReplyMessageRequest,
        PushMessageRequest,
        TextMessage,
        FlexMessage,
        FlexContainer
    )

    config, _ = get_line_config()

    temp_file_path = None

    try:
        # 1. å–å¾—åœ–ç‰‡å…§å®¹
        logger.info(f"[1/4] Downloading image: {message_id}")
        with ApiClient(config) as api_client:
            line_bot_blob_api = MessagingApiBlob(api_client)
            message_content = line_bot_blob_api.get_message_content(message_id)

        # å°‡åœ–ç‰‡å­˜å…¥æš«å­˜æª”
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tf:
            if isinstance(message_content, bytes):
                tf.write(message_content)
            elif hasattr(message_content, 'read'):
                tf.write(message_content.read())
            elif hasattr(message_content, 'content'):
                tf.write(message_content.content)
            else:
                tf.write(bytes(message_content))
            temp_file_path = tf.name

        file_size = os.path.getsize(temp_file_path)
        logger.info(f"[2/4] Image saved: {temp_file_path} ({file_size} bytes)")

        if file_size == 0:
            raise ValueError("Downloaded image is empty (0 bytes)")

        # 2. åˆ†æåœ–ç‰‡ï¼šå…ˆ Geminiï¼Œå¤±æ•—å‰‡ç”¨ OpenRouter å‚™æ´
        logger.info("[3/4] Analyzing image...")
        response_text = None
        used_provider = None

        # --- å˜—è©¦ Gemini ---
        if GEMINI_API_KEYS:
            try:
                logger.info("Trying Gemini first...")
                response = _call_gemini_with_rotation(genai, temp_file_path, ANALYSIS_PROMPT)
                response_text = response.text.strip()
                used_provider = 'Gemini'
            except (QuotaExhaustedError, Exception) as gemini_err:
                logger.warning(f"Gemini failed: {gemini_err}")

        # --- Gemini å¤±æ•—ï¼Œå˜—è©¦ OpenRouter ---
        if response_text is None and OPENROUTER_API_KEY:
            try:
                logger.info("Falling back to OpenRouter...")
                response_text = _call_openrouter_fallback(temp_file_path, ANALYSIS_PROMPT)
                used_provider = 'OpenRouter'
            except Exception as or_err:
                logger.error(f"OpenRouter also failed: {or_err}")

        # --- éƒ½å¤±æ•— ---
        if response_text is None:
            raise Exception("æ‰€æœ‰ AI æœå‹™éƒ½ç„¡æ³•ä½¿ç”¨ï¼ˆGemini + OpenRouterï¼‰")

        logger.info(f"AI response from {used_provider}: {response_text[:200]}")

        # 3. è§£æ JSON
        result_json = _parse_ai_response(response_text)



        # 4. çµ„è£ Flex Message ä¸¦å›å‚³
        logger.info("[4/4] Sending Flex Message...")
        flex_dict = {
            "type": "bubble",
            "header": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": f"ç¬¬ {result_json.get('weeks', '?')} é€±æˆé•·ç´€éŒ„",
                        "weight": "bold",
                        "size": "xl",
                        "color": "#ff7fa8"
                    }
                ]
            },
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {
                        "type": "text",
                        "text": result_json.get('message', 'åª½å’ªå¥½ï¼Œæˆ‘æ˜¯å¯¶å¯¶ï¼'),
                        "wrap": True,
                        "size": "md"
                    }
                ]
            }
        }

        flex_container = FlexContainer.from_dict(flex_dict)
        flex_message = FlexMessage(alt_text="å¯¶å¯¶çš„è¶…éŸ³æ³¢ç´€éŒ„ä¾†å›‰ï¼", contents=flex_container)

        with ApiClient(config) as api_client:
            line_bot_api = MessagingApi(api_client)

            # å…ˆå˜—è©¦ replyï¼ˆå¦‚æœ token é‚„æœ‰æ•ˆï¼‰
            try:
                line_bot_api.reply_message(
                    ReplyMessageRequest(
                        reply_token=reply_token,
                        messages=[flex_message]
                    )
                )
                logger.info("Reply message sent successfully!")
            except Exception as reply_err:
                logger.warning(f"Reply failed ({reply_err}), using push message instead")
                line_bot_api.push_message(
                    PushMessageRequest(
                        to=user_id,
                        messages=[flex_message]
                    )
                )
                logger.info("Push message sent successfully!")

    except Exception as e:
        logger.error(f"Error processing image: {e}", exc_info=True)

        # æ ¹æ“šéŒ¯èª¤é¡å‹çµ¦å‡ºä¸åŒçš„å‹å–„è¨Šæ¯
        if isinstance(e, QuotaExhaustedError):
            user_msg = "å¯¶å¯¶ç¾åœ¨æœ‰é»å¿™ç¢Œï¼Œè«‹éå¹¾åˆ†é˜å†å‚³ä¸€æ¬¡ç…§ç‰‡çµ¦æˆ‘å“¦ ğŸ¼ğŸ’¤"
        elif '429' in str(e) or 'quota' in str(e).lower():
            user_msg = "å¯¶å¯¶ç¾åœ¨æœ‰é»å¿™ç¢Œï¼Œè«‹éå¹¾åˆ†é˜å†å‚³ä¸€æ¬¡ç…§ç‰‡çµ¦æˆ‘å“¦ ğŸ¼ğŸ’¤"
        else:
            user_msg = "æŠ±æ­‰ï¼Œè™•ç†ç…§ç‰‡æ™‚å‡ºäº†é»å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ ğŸ™"

        try:
            with ApiClient(config) as api_client:
                line_bot_api = MessagingApi(api_client)
                line_bot_api.push_message(
                    PushMessageRequest(
                        to=user_id,
                        messages=[TextMessage(text=user_msg)]
                    )
                )
        except Exception as push_err:
            logger.error(f"Failed to send error message: {push_err}")
    finally:
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)
